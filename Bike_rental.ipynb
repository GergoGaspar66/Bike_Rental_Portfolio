{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6db438",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Read and concatenate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import psycopg2\n",
    "\n",
    "# Define the location of CSV files\n",
    "csv_location = \"bike-rental-starter-kit/data\"\n",
    "# Collect all tripdata CSV files from the folder\n",
    "trip_data_files = glob.glob(rf\"{csv_location}/*tripdata.csv\")\n",
    "# Read each CSV file into a DataFrame and concatenate them into a single DataFrame\n",
    "trip_data_df = pd.concat([pd.read_csv(f) for f in trip_data_files], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10c565",
   "metadata": {},
   "source": [
    "Replace whitespace with underscores and convert column names to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trip_data_df.columns = [x.replace(\" \", \"_\").lower() for x in trip_data_df.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc6716",
   "metadata": {},
   "source": [
    "Inspecting first few rows of our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af575be",
   "metadata": {},
   "source": [
    "Inspecting the default datatypes given by Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98da903",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5574b4",
   "metadata": {},
   "source": [
    "Changing to other better suiting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68380bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    \"trip_duration\": \"Int64\",\n",
    "    \"start_station_id\": \"Int64\",\n",
    "    \"end_station_id\": \"Int64\",\n",
    "    \"bike_id\": \"Int64\",\n",
    "    \"birth_year\": \"Int64\",\n",
    "    \"start_station_name\": \"string\",\n",
    "    \"end_station_name\": \"string\",\n",
    "    \"user_type\": \"category\",\n",
    "    \"gender\": \"category\"\n",
    "}\n",
    "trip_data_df = trip_data_df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0b803",
   "metadata": {},
   "source": [
    "Also, we have to convert the date columns separately, because Pandas doesn’t have an astype(\"datetime\") type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df[\"start_time\"] = pd.to_datetime(trip_data_df[\"start_time\"], errors=\"coerce\")\n",
    "trip_data_df[\"stop_time\"] = pd.to_datetime(trip_data_df[\"stop_time\"], errors=\"coerce\")\n",
    "\n",
    "trip_data_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83979f72",
   "metadata": {},
   "source": [
    "Next step, we gonna check, if there are any duplicate rows and if yes we gonna remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57baba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074da16f",
   "metadata": {},
   "source": [
    "We are going to define an ID column using the indexes, so it will be easier to manage our data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b85af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df[\"id\"] = trip_data_df.index\n",
    "trip_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b942ba",
   "metadata": {},
   "source": [
    "Now, let's move forward with further analysis of our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb429a",
   "metadata": {},
   "source": [
    "Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc49df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69943f43",
   "metadata": {},
   "source": [
    "The trip_duration column had a suspiciously large maximum also a very small minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e368f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert trip duration from seconds to minutes and hours\n",
    "trip_data_df[\"trip_minutes\"]=round(trip_data_df[\"trip_duration\"] / 60,2)\n",
    "trip_data_df[\"trip_hours\"]=round(trip_data_df[\"trip_duration\"] / 3600,2)\n",
    "trip_data_df[[\"trip_duration\",\"trip_minutes\",\"trip_hours\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e7eda",
   "metadata": {},
   "source": [
    "According to the Citi Bike data dictionary, any trips shorter than 60 seconds are likely false starts or users quickly re-docking the bike to ensure it’s secure.\n",
    "\n",
    "Looking at the data, some trips are extremely long, e.g., 4,500 hours, which is roughly half a year. It’s plausible that an annual subscriber could have left a bike out for an extended period without returning it. Therefore, we will keep these unusually long trips for analysis.\n",
    "\n",
    "However, trips under 60 seconds will be removed, in line with the official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519134d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.drop(trip_data_df[trip_data_df[\"trip_duration\"] < 60].index, inplace=True)\n",
    "trip_data_df[[\"trip_duration\",\"trip_minutes\",\"trip_hours\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9f77c",
   "metadata": {},
   "source": [
    "After analyzing the data further, we can see that the earliest birth year is 1900. This is quite odd because, considering this dataset was created in 2016, it would mean the cyclist would be over 100 years old. Let's fix this. We are going to drop every row where our cyclist is more than 80 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef605d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.drop(trip_data_df[trip_data_df[\"birth_year\"] < 1936].index, inplace=True)\n",
    "trip_data_df[\"birth_year\"].sort_values(ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c915639",
   "metadata": {},
   "source": [
    "On the other hand, 16-year-olds can easily use a bicycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb84999",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df[\"birth_year\"].sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849df3c",
   "metadata": {},
   "source": [
    "We can also see that there are many missing values in the user_type and birth_year columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf84c3cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trip_data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m missing_birth_year = \u001b[43mtrip_data_df\u001b[49m[trip_data_df[[\u001b[33m\"\u001b[39m\u001b[33mbirth_year\u001b[39m\u001b[33m\"\u001b[39m]].isnull().any(axis=\u001b[32m1\u001b[39m)]\n\u001b[32m      2\u001b[39m missing_birth_year.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'trip_data_df' is not defined"
     ]
    }
   ],
   "source": [
    "missing_birth_year = trip_data_df[trip_data_df[[\"birth_year\"]].isnull().any(axis=1)]\n",
    "missing_birth_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_birth_year[\"user_type\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_birth_year['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_birth_year[\"user_type\"].value_counts() /trip_data_df[\"user_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4346e53",
   "metadata": {},
   "source": [
    "We can see that 99% of the customers don’t have a birth year, and their gender is also unknown. Therefore, it is reasonable to assume that people with missing birth year and unknown gender should be assigned the \"Customer\" role, while the others can be assumed to be \"Subscribers.\" This is because a customer can use a bike for a maximum of three days, whereas a subscriber can use it for a year. It is more likely that annual subscribers have long-term plans and will provide more information about themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90999eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 'customer' to rows where user_type is missing and either gender is 0 or birth_year is missing\n",
    "cond_customer = trip_data_df[\"user_type\"].isna() & ((trip_data_df[\"gender\"] == 0) | trip_data_df[\"birth_year\"].isna())\n",
    "trip_data_df.loc[cond_customer, \"user_type\"] = \"Customer\"\n",
    "\n",
    "# Assign 'subscriber' to rows where user_type is missing but birth_year is known and gender is not 0\n",
    "cond_subscriber = trip_data_df[\"user_type\"].isna() & trip_data_df[\"birth_year\"].notna() & (trip_data_df[\"gender\"] != 0)\n",
    "trip_data_df.loc[cond_subscriber, \"user_type\"] = \"Subscriber\"\n",
    "\n",
    "trip_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d416ce4",
   "metadata": {},
   "source": [
    "Now we only have to deal with the missing birth_year values. There are three possible approaches: we could drop the rows with missing values, fill them with a value such as the average birth year of the DataFrame, or leave them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72300c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df[\"birth_year\"].isna().sum() / len(trip_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc8cd6",
   "metadata": {},
   "source": [
    "Only 7% of the data is missing. We can replace the missing values with an average estimate so that our dataset has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_birth_year = int(trip_data_df[\"birth_year\"].mean())\n",
    "trip_data_df[\"birth_year\"] = trip_data_df[\"birth_year\"].fillna(avg_birth_year)\n",
    "trip_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f883b",
   "metadata": {},
   "source": [
    "Lets look at gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.groupby(['user_type','gender']).count()['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a4fbc",
   "metadata": {},
   "source": [
    "There were only a few rows that we assigned to the Customer user type. This reflects the fact that many customers did not provide complete information. As a result, the Customer data may not be as reliable as the Subscriber data, and we should keep this in mind during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec360e09",
   "metadata": {},
   "source": [
    "Since we dropped a few rows lets reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_df.reset_index(drop=True,inplace=True)\n",
    "trip_data_df['ID'] = trip_data_df.index\n",
    "trip_data_df.drop(\"id\",axis=1,inplace=True)\n",
    "trip_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f4dc5",
   "metadata": {},
   "source": [
    "Now that we have finished cleaning the data, we will connect to a PostgreSQL server (supabase.com) and upload our DataFrame into relational tables.\n",
    "Firstly we create the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# For local usage:\n",
    "    #from dotenv import load_dotenv\n",
    "    #load_dotenv()\n",
    "\n",
    "\n",
    "# Read the password from environment variable\n",
    "password = os.getenv(\"PG_PASSWORD\")\n",
    "#Connection setup with supabase:\n",
    "conn_string = (\n",
    "    f\"postgresql://postgres.juqzmtaoicczafyaspmt:{password}@aws-1-eu-central-1.pooler.supabase.com:6543/postgres\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee4c66",
   "metadata": {},
   "source": [
    "This section creates the necessary PostgreSQL schema and tables for the Bike Rental project. These tables are designed to store stations, users, trips, gender information, and a date dimension for analytics purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with psycopg2.connect(conn_string) as conn:\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"       \n",
    "                CREATE SCHEMA IF NOT EXISTS bikeshare;\n",
    "\n",
    "                CREATE TABLE IF NOT EXISTS bikeshare.stations (\n",
    "                    id INT PRIMARY KEY,\n",
    "                    name VARCHAR(100),\n",
    "                    latitude DECIMAL,\n",
    "                    longitude DECIMAL\n",
    "                );\n",
    "\n",
    "                CREATE TABLE IF NOT EXISTS bikeshare.gender (\n",
    "                    id INT PRIMARY KEY,\n",
    "                    gender_name VARCHAR(10)\n",
    "                );\n",
    "\n",
    "                CREATE TABLE IF NOT EXISTS bikeshare.users (\n",
    "                    id INT PRIMARY KEY,\n",
    "                    user_type VARCHAR(30),\n",
    "                    gender INT REFERENCES bikeshare.gender(id),\n",
    "                    birth_year INT\n",
    "                );\n",
    "                  CREATE TABLE IF NOT EXISTS bikeshare.dim_date (\n",
    "                    date_id INT PRIMARY KEY,    \n",
    "                    full_date DATE NOT NULL,      \n",
    "                    year INT,\n",
    "                    month INT,\n",
    "                    day INT,\n",
    "                    month_name VARCHAR(20),\n",
    "                    day_name VARCHAR(20),\n",
    "                    quarter INT\n",
    "                );\n",
    "                                \n",
    "                CREATE TABLE IF NOT EXISTS bikeshare.trip_informations (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    trip_duration INT,\n",
    "                    start_time TIMESTAMP,\n",
    "                    stop_time TIMESTAMP,\n",
    "                    start_station_id INT REFERENCES bikeshare.stations(id),\n",
    "                    end_station_id INT REFERENCES bikeshare.stations(id),\n",
    "                    bike_id INT,\n",
    "                    user_id INT REFERENCES bikeshare.users(id),\n",
    "                    date_key INT REFERENCES bikeshare.dim_date(date_id)    \n",
    "                );\n",
    "            \"\"\")\n",
    "            print(\"Tables created successfully.\")\n",
    "    except psycopg2.DatabaseError as e:\n",
    "        print(\"Database error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccdae96",
   "metadata": {},
   "source": [
    "Now we are going to define a new dataset to build the Date Dimension table for our database. This is very useful because it standardizes all date attributes across the data warehouse, making it easier to perform consistent time-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3db7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "beginning = datetime.datetime(2016,1,1)\n",
    "col_date = [beginning + datetime.timedelta(x) for x in range(366)]\n",
    "date_df = pd.DataFrame(col_date, columns=[\"full_date\"])\n",
    "date_df[\"full_date\"] = pd.to_datetime(date_df[\"full_date\"], errors=\"coerce\")\n",
    "date_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6323b",
   "metadata": {},
   "source": [
    "Now we are going to generate the year, month, day, month name, day name and the quarter of the year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b19aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df[\"year\"]=date_df[\"full_date\"].dt.year\n",
    "date_df[\"month\"] = date_df[\"full_date\"].dt.month\n",
    "date_df[\"day\"] = date_df[\"full_date\"].dt.day\n",
    "date_df[\"month_name\"] = date_df[\"full_date\"].dt.strftime(\"%B\")\n",
    "date_df[\"day_name\"] = date_df[\"full_date\"].dt.strftime(\"%A\")\n",
    "date_df[\"quarter\"] = date_df[\"full_date\"].dt.quarter\n",
    "date_df.head()\n",
    "date_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a866f2a",
   "metadata": {},
   "source": [
    "Next, we are going to create the two keys that will join these two future tables together in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a1d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_df[\"date_id\"] = date_df[\"full_date\"].dt.strftime(\"%Y%m%d\").astype(\"int64\")\n",
    "trip_data_df[\"date_key\"] = trip_data_df[\"start_time\"].dt.strftime(\"%Y%m%d\").astype(\"int64\")\n",
    "date_df=date_df[[\"date_id\",\"full_date\",\"year\",\"month\",\"day\",\"month_name\",\"day_name\",\"quarter\"]]\n",
    "date_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00c8c3",
   "metadata": {},
   "source": [
    "“We will also create a separate dataframe for the stations, so that we can upload them into the stations table in our SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stations_df=trip_data_df[[\"start_station_id\",\"start_station_name\",\"start_station_latitude\",\"start_station_longitude\"]].drop_duplicates().reset_index(drop=True)\n",
    "end_stations_df = trip_data_df[[\"end_station_id\",\"end_station_name\",\"end_station_latitude\",\"end_station_longitude\"]].drop_duplicates().reset_index(drop=True)\n",
    "start_stations_df.columns = [\"station_id\", \"station_name\", \"latitude\", \"longitude\"]\n",
    "end_stations_df.columns = [\"station_id\", \"station_name\", \"latitude\", \"longitude\"]\n",
    "stations_df=pd.concat([start_stations_df,end_stations_df]).drop_duplicates().reset_index(drop=True)\n",
    "stations_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac95204",
   "metadata": {},
   "source": [
    "In order to ensure each user is unique, we extracted the \"birth_year\", \"user_type\", and \"gender\" columns into a separate dataframe, dropped any duplicate rows, added a user_id column, and then merged it back with the original dataframe. This way, each user now has a unique identifier for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df=trip_data_df[[\"birth_year\",\"user_type\",\"gender\"]].drop_duplicates().reset_index(drop=True)\n",
    "users_df[\"user_id\"]=range(1,len(users_df)+1)\n",
    "if \"user_id\" not in trip_data_df.columns:\n",
    "    trip_data_df=trip_data_df.merge(users_df, on= [\"birth_year\",\"user_type\",\"gender\"],how=\"left\")\n",
    "users_df.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2890b",
   "metadata": {},
   "source": [
    "Now we are going to use the SQL INSERT command to populate our tables with the cleaned DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ca5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Próbáld ki az AI-t közvetlenül kedvenc alkalmazásaidban … A Gemini segítségével vázlatokat hozhatsz létre, és finomíthatod a tartalmakat. Ezenkívül a Gemini Pro szolgáltatással 1 hónapig 8790 Ft 0 Ft díj (személyre szabott ár) ellenében használhatod a Google következő generációs AI-technológiáját.\n",
    "from psycopg2.py\n",
    "from psycopg2.extras import execute_values\n",
    "from psycopg2 import sql\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "def insert_data(table_name, columns, dataframe, conn_string):\n",
    "    \"\"\"\n",
    "    Inserts all rows from a pandas DataFrame into a PostgreSQL table.\n",
    "\n",
    "    table_name: str, name of the table\n",
    "    columns: tuple of column names e.g. (\"id\", \"name\", \"latitude\")\n",
    "    dataframe: pandas DataFrame\n",
    "    con_string: psycopg2 connection parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with psycopg2.connect(conn_string) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "\n",
    "                # Count rows before\n",
    "                cur.execute(\n",
    "                    sql.SQL(\"SELECT COUNT(*) FROM {} ;\").format(sql.Identifier(table_name)))\n",
    "                before = cur.fetchone()[0]\n",
    "\n",
    "                # Convert all types to native Python types\n",
    "                values = [\n",
    "                    tuple(v.item() if hasattr(v, \"item\") else v for v in row)\n",
    "                    for row in dataframe.itertuples(index=False)\n",
    "                ]\n",
    "\n",
    "                # Prepare SQL\n",
    "                safe_col = sql.SQL(\", \").join(\n",
    "                    [sql.Identifier(x)for x in columns])\n",
    "                query = sql.SQL(\"INSERT INTO {table_name} ({col_names}) VALUES %s\").format(\n",
    "                    table_name=sql.Identifier(table_name),\n",
    "                    col_names=safe_col\n",
    "                )\n",
    "\n",
    "                # Bulk insert\n",
    "                execute_values(cur, query, values)\n",
    "\n",
    "                # Count rows after\n",
    "                cur.execute(\n",
    "                    sql.SQL(\"SELECT COUNT(*) FROM {};\".format(sql.Identifier(table_name))))\n",
    "                after = cur.fetchone()[0]\n",
    "\n",
    "                inserted = after - before\n",
    "                if inserted != len(values):\n",
    "                    raise psycopg2.DatabaseError(\n",
    "                        f\"Expected {len(values)} rows inserted, got {inserted}\"\n",
    "                    )\n",
    "\n",
    "                print(f\"Rows inserted: {inserted}\")\n",
    "\n",
    "    except psycopg2.DatabaseError as e:\n",
    "        print(\"Database error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548870ff",
   "metadata": {},
   "source": [
    "To facilitate iterative testing and streamline repeated executions during the development of this project, I implemented a routine to reset and truncate all relevant database tables efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ac4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                TRUNCATE TABLE bikeshare.trip_informations RESTART IDENTITY CASCADE;\n",
    "                TRUNCATE TABLE bikeshare.users RESTART IDENTITY CASCADE;\n",
    "                TRUNCATE TABLE bikeshare.stations RESTART IDENTITY CASCADE;\n",
    "                TRUNCATE TABLE bikeshare.dim_date RESTART IDENTITY CASCADE;\n",
    "                TRUNCATE TABLE bikeshare.gender RESTART IDENTITY CASCADE;\n",
    "            \"\"\")\n",
    "            print(\"All tables truncated successfully.\")\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(\"Database error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c073b8",
   "metadata": {},
   "source": [
    "“We will leverage the insert_data function to efficiently populate the database tables with our cleaned and structured datasets, ensuring data integrity and consistency across the Databse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations table\n",
    "insert_data(\n",
    "    \"bikeshare.stations\",\n",
    "    (\"id\", \"name\", \"latitude\", \"longitude\"),\n",
    "    stations_df,\n",
    "    conn_string\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender table\n",
    "try:\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO bikeshare.gender (id, gender_name)\n",
    "                VALUES\n",
    "                    (0, 'Unknown'),\n",
    "                    (1, 'Male'),\n",
    "                    (2, 'Female');\n",
    "            \"\"\")\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f071c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#users table\n",
    "insert_data(\n",
    "    \"bikeshare.users\",\n",
    "    (\"birth_year\",\"user_type\",\"gender\",\"id\"),\n",
    "    users_df,\n",
    "    conn_string\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51dff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_date table\n",
    "insert_data(\n",
    "    \"bikeshare.dim_date\",\n",
    "    (\"date_id\", \"full_date\", \"year\", \"month\", \"day\", \"month_name\", \"day_name\", \"quarter\"),\n",
    "    date_df,\n",
    "    conn_string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97604962",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "For the trip_informations column we create a new datafrom so we can isnert it into our insert_data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_inf_df=trip_data_df[[\"trip_duration\",\"start_time\",\"stop_time\" ,\"start_station_id\"\t,\"end_station_id\",\"bike_id\",\"user_id\",\"date_key\"]].copy()\n",
    "# trip_informations table\n",
    "insert_data(\n",
    "    \"bikeshare.trip_informations\",\n",
    "    (\"trip_duration\",\"start_time\",\"stop_time\" ,\"start_station_id\",\"end_station_id\",\"bike_id\",\"user_id\",\"date_key\"),\n",
    "    trip_inf_df,\n",
    "    conn_string\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b9a2d",
   "metadata": {},
   "source": [
    "Creating views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "\n",
    "            #You can see which bikes are the most popular, how long they are used on average, and the total time each bike has been ridden\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE OR REPLACE VIEW bikeshare.bike_usage AS\n",
    "                SELECT \n",
    "                    bike_id,\n",
    "                    COUNT(id) AS total_trips,\n",
    "                    ROUND(AVG(trip_duration)::numeric, 2) AS avg_trip_duration,\n",
    "                    ROUND(SUM(trip_duration)::numeric, 2) AS total_duration\n",
    "                FROM bikeshare.trip_informations\n",
    "                GROUP BY bike_id;\n",
    "            \"\"\")\n",
    "\n",
    "            #Useful to understand user behavior, identify heavy users, and spot patterns across different user types.\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE OR REPLACE VIEW bikeshare.user_trip_stats AS\n",
    "                SELECT \n",
    "                    u.id AS user_id,\n",
    "                    u.user_type,\n",
    "                    g.gender_name,\n",
    "                    COUNT(t.id) AS total_trips,\n",
    "                    ROUND(AVG(t.trip_duration)::numeric, 2) AS avg_trip_duration,\n",
    "                    ROUND(SUM(t.trip_duration)::numeric, 2) AS total_duration\n",
    "                FROM bikeshare.users u\n",
    "                LEFT JOIN bikeshare.gender g ON u.gender = g.id\n",
    "                LEFT JOIN bikeshare.trip_informations t ON t.user_id = u.id\n",
    "                GROUP BY u.id, u.user_type, g.gender_name;\n",
    "            \"\"\")\n",
    "            #Helps with station capacity planning and identifying busy or underused stations.\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE OR REPLACE VIEW bikeshare.station_usage AS\n",
    "                SELECT \n",
    "                    s.id AS station_id,\n",
    "                    s.name AS station_name,\n",
    "                    COUNT(t.id) FILTER (WHERE t.start_station_id = s.id) AS trips_started,\n",
    "                    COUNT(t.id) FILTER (WHERE t.end_station_id = s.id) AS trips_ended\n",
    "                FROM bikeshare.stations s\n",
    "                LEFT JOIN bikeshare.trip_informations t\n",
    "                    ON t.start_station_id = s.id OR t.end_station_id = s.id\n",
    "                GROUP BY s.id, s.name;\n",
    "            \"\"\")\n",
    "            #Offers a complete view of the system’s activity for reporting or analytics purposes.\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE OR REPLACE VIEW bikeshare.trip_summary AS\n",
    "                SELECT \n",
    "                    t.id AS trip_id,\n",
    "                    u.id AS user_id,\n",
    "                    u.user_type,\n",
    "                    g.gender_name,\n",
    "                    u.birth_year,\n",
    "                    ROUND(t.trip_duration::numeric, 2) AS trip_duration,\n",
    "                    t.start_time,\n",
    "                    t.stop_time,\n",
    "                    s_start.name AS start_station,\n",
    "                    s_end.name AS end_station,\n",
    "                    t.bike_id\n",
    "                FROM bikeshare.trip_informations t\n",
    "                JOIN bikeshare.users u ON t.user_id = u.id\n",
    "                JOIN bikeshare.gender g ON u.gender = g.id\n",
    "                JOIN bikeshare.stations s_start ON t.start_station_id = s_start.id\n",
    "                JOIN bikeshare.stations s_end ON t.end_station_id = s_end.id;\n",
    "            \"\"\")\n",
    "            #Summarizes daily bike trips by date, showing total trips, average and total trip duration, and unique users.\n",
    "\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE OR REPLACE VIEW bikeshare.daily_trip_summary AS\n",
    "                SELECT\n",
    "                    d.date_id,\n",
    "                    d.full_date,\n",
    "                    d.year,\n",
    "                    d.month,\n",
    "                    d.day,\n",
    "                    d.month_name,\n",
    "                    d.day_name,\n",
    "                    d.quarter,\n",
    "                    COUNT(t.id) AS total_trips,\n",
    "                    CAST(ROUND(AVG(t.trip_duration)::numeric, 2) AS numeric) AS avg_trip_duration,\n",
    "                    CAST(ROUND(SUM(t.trip_duration)::numeric, 2) AS numeric) AS total_trip_duration,\n",
    "                    COUNT(DISTINCT t.user_id) AS unique_users\n",
    "                FROM bikeshare.dim_date d\n",
    "                LEFT JOIN bikeshare.trip_informations t\n",
    "                    ON t.date_key = d.date_id\n",
    "                GROUP BY\n",
    "                    d.date_id, d.full_date, d.year, d.month, d.day, d.month_name, d.day_name, d.quarter\n",
    "                ORDER BY d.full_date;\n",
    "\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "            print(\"All views created\")\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(\"Database error:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
